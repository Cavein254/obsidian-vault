Hyper-parameter are parameters that cannot be updated during training of ML. They provide structure to the model. 
Sigmoid activation function although the most popular activation functions used of recent they are falling in popularity
$$f(x) = \frac{1}{1 + e^{-x}}$$
Their major disadvantage the output of $f(x)$ corresponds very little to the input of $x$ giving rise to the problem of vanishing gradient. 